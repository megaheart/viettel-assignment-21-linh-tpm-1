{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push data from log_action.csv to kafka topic vdt2024 using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confluent_kafka in /opt/conda/lib/python3.10/site-packages (2.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: kafka-python in /opt/conda/lib/python3.10/site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install confluent_kafka\n",
    "%pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message sent\n",
      "Failed to deliver message: <cimpl.Message object at 0x7f23f5c616c0>: KafkaError{code=_MSG_TIMED_OUT,val=-192,str=\"Local: Message timed out\"}\n",
      "message flush\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "import socket\n",
    "import time\n",
    "import io\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "producer = Producer({\n",
    "\n",
    "    'bootstrap.servers': 'broker01:9092',\n",
    "    'client.id': socket.gethostname(),\n",
    "\n",
    "    \"batch.size\": 1,\n",
    "\n",
    "})\n",
    "\n",
    "topic = 'vdt2024'\n",
    "\n",
    "\n",
    "\n",
    "def acked(err, msg):\n",
    "\n",
    "    if err is not None:\n",
    "\n",
    "        print(\"Failed to deliver message: %s: %s\" % (str(msg), str(err)))\n",
    "    else:\n",
    "\n",
    "        print(\"Message produced: %s\" % (str(msg)))\n",
    "\n",
    "\n",
    "\n",
    "with open('./data/log_action.csv') as csv_file:\n",
    "\n",
    "    idx = 0\n",
    "\n",
    "    for line in csv_file:\n",
    "\n",
    "        fields = line.replace('\\n', '').split(',')\n",
    "\n",
    "        data_set = {\n",
    "\n",
    "            \"student_code\": int(fields[0]),\n",
    "\n",
    "            \"activity\": fields[1],\n",
    "\n",
    "            \"numberOfFile\": int(fields[2]),\n",
    "\n",
    "            \"timestamp\": fields[3]\n",
    "\n",
    "        }\n",
    "\n",
    "        producer.produce(topic, key=str(idx), value=json.dumps(data_set), callback=acked)\n",
    "        print('message sent')\n",
    "\n",
    "        producer.flush()\n",
    "        print('message flush')\n",
    "        break\n",
    "\n",
    "\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "action_schema = StructType().add(\"student_code\", \"integer\")\\\n",
    "    .add(\"activity\", \"string\")\\\n",
    "    .add(\"numberOfFile\", \"integer\")\\\n",
    "    .add(\"timestamp\", \"string\")\n",
    "\n",
    "sv_de_schema = StructType().add(\"student_code\", \"integer\")\\\n",
    "    .add(\"fullname\", \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save danh_sach_sv_de to hdfs with path: /raw_zone/danh_sach_sv_de.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|student_code|            fullname|\n",
      "+------------+--------------------+\n",
      "|           1|          Mai Đức An|\n",
      "|           2|      Nguyễn Mai Anh|\n",
      "|           3|   Ngô Ngọc Tuấn Anh|\n",
      "|           4|      Trần Trung Anh|\n",
      "|           5|       Trần Ngọc Bảo|\n",
      "|           6|  Nguyễn Vũ Hòa Bình|\n",
      "|           7|    Nguyễn Thành Đạt|\n",
      "|           8|        Đỗ Thành Đạt|\n",
      "|           9|    Nguyễn Khoa Đoàn|\n",
      "|          10|    Nguyễn Quốc Dũng|\n",
      "|          11|     Đường Minh Quân|\n",
      "|          12|   Dương Quang Giang|\n",
      "|          13|    Nguyễn Minh Hiếu|\n",
      "|          14|        Ngô Phi Hùng|\n",
      "|          15|Nguyễn Đình Thiên...|\n",
      "|          16|        Đỗ Doãn Khắc|\n",
      "|          17|      Châu Minh Khải|\n",
      "|          18|      Phạm Đình Khôi|\n",
      "|          19|        Lê Bảo Khánh|\n",
      "|          20|        Lê Minh Phúc|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"false\").schema(sv_de_schema)\\\n",
    "    .load(\"./data/danh_sach_sv_de.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"csv\").option(\"header\", \"false\").mode(\"overwrite\")\\\n",
    "    .save(\"hdfs://namenode:9000/raw_zone/danh_sach_sv_de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check data in hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: hdfs://namenode:9000/raw_zone/danh_sach_sv_de",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m sv_de_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfalse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43msv_de_schema\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs://namenode:9000/raw_zone/danh_sach_sv_de\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m sv_de_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m40\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:158\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: hdfs://namenode:9000/raw_zone/danh_sach_sv_de"
     ]
    }
   ],
   "source": [
    "sv_de_df = spark.read.format(\"csv\").option(\"header\", \"false\").schema(sv_de_schema)\\\n",
    "    .load(\"hdfs://namenode:9000/raw_zone/danh_sach_sv_de\")\n",
    "sv_de_df.show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_data = spark.read.csv(\"hdfs://namenode:9000/user/hadoop/data/action_data.csv\",\n",
    "                             header=True, schema=action_schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
